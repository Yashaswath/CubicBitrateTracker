{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iGkL-KtJhWt",
        "outputId": "4095d0f8-2de2-424c-b7aa-978ec83f40b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8h2-hYom2K1"
      },
      "source": [
        "Code for generating bitrates of an equirectangular video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb7mT1NOrh31",
        "outputId": "9fe42e76-2946-496d-d762-5636e69c09ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'tile_1_left.mp4': 693120, 'tile_2_left.mp4': 527520, 'tile_3_left.mp4': 545865, 'tile_4_left.mp4': 524865, 'tile_5_left.mp4': 553365, 'tile_6_left.mp4': 540645, 'tile_1_right.mp4': 535320, 'tile_2_right.mp4': 445740, 'tile_3_right.mp4': 545865, 'tile_4_right.mp4': 441990, 'tile_5_right.mp4': 530700, 'tile_6_right.mp4': 540645}\n",
            "{'tile_1_left.mp4': 665632, 'tile_2_left.mp4': 533808, 'tile_3_left.mp4': 561792, 'tile_4_left.mp4': 506288, 'tile_5_left.mp4': 560960, 'tile_6_left.mp4': 553920, 'tile_1_right.mp4': 549248, 'tile_2_right.mp4': 456736, 'tile_3_right.mp4': 561792, 'tile_4_right.mp4': 449536, 'tile_5_right.mp4': 542944, 'tile_6_right.mp4': 553920}\n",
            "{'tile_1_left.mp4': 1072848, 'tile_2_left.mp4': 809488, 'tile_3_left.mp4': 763472, 'tile_4_left.mp4': 806064, 'tile_5_left.mp4': 922544, 'tile_6_left.mp4': 829840, 'tile_1_right.mp4': 823552, 'tile_2_right.mp4': 654640, 'tile_3_right.mp4': 763472, 'tile_4_right.mp4': 809200, 'tile_5_right.mp4': 765776, 'tile_6_right.mp4': 829840}\n",
            "{'tile_1_left.mp4': 996688, 'tile_2_left.mp4': 792672, 'tile_3_left.mp4': 726016, 'tile_4_left.mp4': 775056, 'tile_5_left.mp4': 886112, 'tile_6_left.mp4': 792848, 'tile_1_right.mp4': 799136, 'tile_2_right.mp4': 605152, 'tile_3_right.mp4': 726016, 'tile_4_right.mp4': 750080, 'tile_5_right.mp4': 753376, 'tile_6_right.mp4': 792848}\n",
            "{'tile_1_left.mp4': 1018912, 'tile_2_left.mp4': 806816, 'tile_3_left.mp4': 751760, 'tile_4_left.mp4': 779600, 'tile_5_left.mp4': 918976, 'tile_6_left.mp4': 840928, 'tile_1_right.mp4': 835808, 'tile_2_right.mp4': 658640, 'tile_3_right.mp4': 751760, 'tile_4_right.mp4': 739072, 'tile_5_right.mp4': 769552, 'tile_6_right.mp4': 840928}\n",
            "{'tile_1_left.mp4': 1028496, 'tile_2_left.mp4': 827968, 'tile_3_left.mp4': 782352, 'tile_4_left.mp4': 796896, 'tile_5_left.mp4': 906720, 'tile_6_left.mp4': 833648, 'tile_1_right.mp4': 843456, 'tile_2_right.mp4': 704672, 'tile_3_right.mp4': 782352, 'tile_4_right.mp4': 742288, 'tile_5_right.mp4': 813488, 'tile_6_right.mp4': 833648}\n",
            "{'tile_1_left.mp4': 1015920, 'tile_2_left.mp4': 828672, 'tile_3_left.mp4': 787216, 'tile_4_left.mp4': 773328, 'tile_5_left.mp4': 900912, 'tile_6_left.mp4': 860352, 'tile_1_right.mp4': 893568, 'tile_2_right.mp4': 695200, 'tile_3_right.mp4': 787216, 'tile_4_right.mp4': 718640, 'tile_5_right.mp4': 840080, 'tile_6_right.mp4': 860352}\n",
            "{'tile_1_left.mp4': 1007120, 'tile_2_left.mp4': 825488, 'tile_3_left.mp4': 780480, 'tile_4_left.mp4': 761008, 'tile_5_left.mp4': 884592, 'tile_6_left.mp4': 828048, 'tile_1_right.mp4': 957648, 'tile_2_right.mp4': 670192, 'tile_3_right.mp4': 780480, 'tile_4_right.mp4': 720544, 'tile_5_right.mp4': 811856, 'tile_6_right.mp4': 828048}\n",
            "{'tile_1_left.mp4': 946736, 'tile_2_left.mp4': 774928, 'tile_3_left.mp4': 706080, 'tile_4_left.mp4': 757200, 'tile_5_left.mp4': 824944, 'tile_6_left.mp4': 799616, 'tile_1_right.mp4': 940832, 'tile_2_right.mp4': 621392, 'tile_3_right.mp4': 706080, 'tile_4_right.mp4': 679008, 'tile_5_right.mp4': 769808, 'tile_6_right.mp4': 799616}\n",
            "{'tile_1_left.mp4': 995696, 'tile_2_left.mp4': 796192, 'tile_3_left.mp4': 762016, 'tile_4_left.mp4': 777136, 'tile_5_left.mp4': 862256, 'tile_6_left.mp4': 883456, 'tile_1_right.mp4': 1035696, 'tile_2_right.mp4': 698704, 'tile_3_right.mp4': 762016, 'tile_4_right.mp4': 695728, 'tile_5_right.mp4': 794720, 'tile_6_right.mp4': 883456}\n",
            "{'tile_1_left.mp4': 973952, 'tile_2_left.mp4': 779248, 'tile_3_left.mp4': 731088, 'tile_4_left.mp4': 756736, 'tile_5_left.mp4': 827888, 'tile_6_left.mp4': 909280, 'tile_1_right.mp4': 1081536, 'tile_2_right.mp4': 620112, 'tile_3_right.mp4': 731088, 'tile_4_right.mp4': 669520, 'tile_5_right.mp4': 782800, 'tile_6_right.mp4': 909280}\n",
            "{'tile_1_left.mp4': 995824, 'tile_2_left.mp4': 808160, 'tile_3_left.mp4': 719952, 'tile_4_left.mp4': 768208, 'tile_5_left.mp4': 849888, 'tile_6_left.mp4': 1075040, 'tile_1_right.mp4': 1059472, 'tile_2_right.mp4': 608496, 'tile_3_right.mp4': 719952, 'tile_4_right.mp4': 706480, 'tile_5_right.mp4': 855408, 'tile_6_right.mp4': 1075040}\n",
            "{'tile_1_left.mp4': 989536, 'tile_2_left.mp4': 771008, 'tile_3_left.mp4': 713280, 'tile_4_left.mp4': 761600, 'tile_5_left.mp4': 818256, 'tile_6_left.mp4': 1200272, 'tile_1_right.mp4': 1026496, 'tile_2_right.mp4': 645024, 'tile_3_right.mp4': 713280, 'tile_4_right.mp4': 695184, 'tile_5_right.mp4': 862864, 'tile_6_right.mp4': 1200272}\n",
            "{'tile_1_left.mp4': 987344, 'tile_2_left.mp4': 753648, 'tile_3_left.mp4': 698624, 'tile_4_left.mp4': 795488, 'tile_5_left.mp4': 809360, 'tile_6_left.mp4': 1400560, 'tile_1_right.mp4': 974240, 'tile_2_right.mp4': 578528, 'tile_3_right.mp4': 698624, 'tile_4_right.mp4': 671792, 'tile_5_right.mp4': 989168, 'tile_6_right.mp4': 1400560}\n",
            "{'tile_1_left.mp4': 978512, 'tile_2_left.mp4': 762560, 'tile_3_left.mp4': 716240, 'tile_4_left.mp4': 876912, 'tile_5_left.mp4': 771328, 'tile_6_left.mp4': 1343344, 'tile_1_right.mp4': 1012080, 'tile_2_right.mp4': 583328, 'tile_3_right.mp4': 716240, 'tile_4_right.mp4': 658416, 'tile_5_right.mp4': 1172256, 'tile_6_right.mp4': 1343344}\n",
            "{'tile_1_left.mp4': 957840, 'tile_2_left.mp4': 775312, 'tile_3_left.mp4': 710128, 'tile_4_left.mp4': 955888, 'tile_5_left.mp4': 784080, 'tile_6_left.mp4': 1337152, 'tile_1_right.mp4': 1029648, 'tile_2_right.mp4': 577984, 'tile_3_right.mp4': 710128, 'tile_4_right.mp4': 673776, 'tile_5_right.mp4': 1320208, 'tile_6_right.mp4': 1337152}\n",
            "{'tile_1_left.mp4': 1001680, 'tile_2_left.mp4': 800160, 'tile_3_left.mp4': 736016, 'tile_4_left.mp4': 1069200, 'tile_5_left.mp4': 798464, 'tile_6_left.mp4': 1135808, 'tile_1_right.mp4': 1132288, 'tile_2_right.mp4': 621248, 'tile_3_right.mp4': 736016, 'tile_4_right.mp4': 678976, 'tile_5_right.mp4': 1147088, 'tile_6_right.mp4': 1135808}\n",
            "{'tile_1_left.mp4': 951632, 'tile_2_left.mp4': 769328, 'tile_3_left.mp4': 706176, 'tile_4_left.mp4': 992464, 'tile_5_left.mp4': 775888, 'tile_6_left.mp4': 1013328, 'tile_1_right.mp4': 1175216, 'tile_2_right.mp4': 583456, 'tile_3_right.mp4': 706176, 'tile_4_right.mp4': 662496, 'tile_5_right.mp4': 1052768, 'tile_6_right.mp4': 1013328}\n",
            "{'tile_1_left.mp4': 929456, 'tile_2_left.mp4': 692448, 'tile_3_left.mp4': 692768, 'tile_4_left.mp4': 895904, 'tile_5_left.mp4': 746816, 'tile_6_left.mp4': 950064, 'tile_1_right.mp4': 1147600, 'tile_2_right.mp4': 565680, 'tile_3_right.mp4': 692768, 'tile_4_right.mp4': 702816, 'tile_5_right.mp4': 892720, 'tile_6_right.mp4': 950064}\n",
            "{'tile_1_left.mp4': 921120, 'tile_2_left.mp4': 702784, 'tile_3_left.mp4': 697280, 'tile_4_left.mp4': 1027424, 'tile_5_left.mp4': 751168, 'tile_6_left.mp4': 1080448, 'tile_1_right.mp4': 1107136, 'tile_2_right.mp4': 559552, 'tile_3_right.mp4': 697280, 'tile_4_right.mp4': 793040, 'tile_5_right.mp4': 696080, 'tile_6_right.mp4': 1080448}\n",
            "{'tile_1_left.mp4': 2757600, 'tile_2_left.mp4': 2000280, 'tile_3_left.mp4': 2242800, 'tile_4_left.mp4': 2220720, 'tile_5_left.mp4': 1974600, 'tile_6_left.mp4': 2603040, 'tile_1_right.mp4': 2485560, 'tile_2_right.mp4': 1850040, 'tile_3_right.mp4': 2242800, 'tile_4_right.mp4': 1763760, 'tile_5_right.mp4': 2145720, 'tile_6_right.mp4': 2603040}\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[-27488, 6288, 15927, -18577, 7595, 13275, 13928, 10996, 15927, 7546, 12244, 13275]\n",
            "[407216, 275680, 201680, 299776, 361584, 275920, 274304, 197904, 201680, 359664, 222832, 275920]\n",
            "[-76160, -16816, -37456, -31008, -36432, -36992, -24416, -49488, -37456, -59120, -12400, -36992]\n",
            "[22224, 14144, 25744, 4544, 32864, 48080, 36672, 53488, 25744, -11008, 16176, 48080]\n",
            "[9584, 21152, 30592, 17296, -12256, -7280, 7648, 46032, 30592, 3216, 43936, -7280]\n",
            "[-12576, 704, 4864, -23568, -5808, 26704, 50112, -9472, 4864, -23648, 26592, 26704]\n",
            "[-8800, -3184, -6736, -12320, -16320, -32304, 64080, -25008, -6736, 1904, -28224, -32304]\n",
            "[-60384, -50560, -74400, -3808, -59648, -28432, -16816, -48800, -74400, -41536, -42048, -28432]\n",
            "[48960, 21264, 55936, 19936, 37312, 83840, 94864, 77312, 55936, 16720, 24912, 83840]\n",
            "[-21744, -16944, -30928, -20400, -34368, 25824, 45840, -78592, -30928, -26208, -11920, 25824]\n",
            "[21872, 28912, -11136, 11472, 22000, 165760, -22064, -11616, -11136, 36960, 72608, 165760]\n",
            "[-6288, -37152, -6672, -6608, -31632, 125232, -32976, 36528, -6672, -11296, 7456, 125232]\n",
            "[-2192, -17360, -14656, 33888, -8896, 200288, -52256, -66496, -14656, -23392, 126304, 200288]\n",
            "[-8832, 8912, 17616, 81424, -38032, -57216, 37840, 4800, 17616, -13376, 183088, -57216]\n",
            "[-20672, 12752, -6112, 78976, 12752, -6192, 17568, -5344, -6112, 15360, 147952, -6192]\n",
            "[43840, 24848, 25888, 113312, 14384, -201344, 102640, 43264, 25888, 5200, -173120, -201344]\n",
            "[-50048, -30832, -29840, -76736, -22576, -122480, 42928, -37792, -29840, -16480, -94320, -122480]\n",
            "[-22176, -76880, -13408, -96560, -29072, -63264, -27616, -17776, -13408, 40320, -160048, -63264]\n",
            "[-8336, 10336, 4512, 131520, 4352, 130384, -40464, -6128, 4512, 90224, -196640, 130384]\n",
            "[1836480, 1297496, 1545520, 1193296, 1223432, 1522592, 1378424, 1290488, 1545520, 970720, 1449640, 1522592]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "\"\"\"Change the input name of the file here\"\"\"\n",
        "input_video = \"/content/drive/MyDrive/360_Video_IS/360_london_4k_10s.mp4\"\n",
        "\n",
        "# Function to get video duration\n",
        "def get_video_duration(file_path):\n",
        "    result = subprocess.run(\n",
        "        [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v:0\", \"-show_entries\", \"stream=duration\", \"-of\", \"csv=p=0\", file_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    return float(result.stdout.strip())\n",
        "\n",
        "def get_video_segements(file_path,segement_dir,segement_time=0.5):\n",
        "    # Ensure segments directory exists\n",
        "    os.makedirs(output_segments_dir, exist_ok=True)\n",
        "    command = [\n",
        "    \"ffmpeg\",\n",
        "    \"-i\", f\"{file_path}\",\n",
        "    \"-c:v\", \"libx264\",\n",
        "    \"-c:a\", \"aac\",\n",
        "    \"-strict\", \"experimental\",\n",
        "    \"-b:a\", \"192k\",\n",
        "    \"-force_key_frames\", f\"expr:gte(t,n_forced*{segement_time})\",\n",
        "    \"-f\", \"segment\",\n",
        "    \"-segment_time\", f\"{segement_time}\",\n",
        "    \"-reset_timestamps\", \"1\",\n",
        "    \"-map\", \"0\",\n",
        "    f\"{segement_dir}/output_part_%d.mp4\"\n",
        "    ]\n",
        "    subprocess.run(command,check=True)\n",
        "\n",
        "#Function to convert an equirectangular to cubic\n",
        "def convert_video_to_cubemap(input_file, input_format=\"equirect\", output_format=\"c3x2\", quality=2):\n",
        "    \"\"\"\n",
        "    Converts a video from equirectangular projection to cubemap projection using FFmpeg.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the input video file.\n",
        "        input_format (str): Input projection format (default is 'equirect').\n",
        "        output_format (str): Output projection format (default is 'c3x2').\n",
        "        quality (int): Video quality parameter (lower is better quality, default is 2).\n",
        "\n",
        "    Returns:\n",
        "        str: The FFmpeg command executed.\n",
        "    \"\"\"\n",
        "    # Derive output file name\n",
        "    base_name, ext = os.path.splitext(input_file)\n",
        "    output_file = f\"{base_name}_cube{ext}\"\n",
        "\n",
        "    command = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\", input_file,\n",
        "        \"-vf\", f\"v360=input={input_format}:output={output_format}\",\n",
        "        \"-q:v\", str(quality),\n",
        "        output_file\n",
        "    ]\n",
        "\n",
        "    # Execute the FFmpeg command\n",
        "    subprocess.run(command, check=True)\n",
        "\n",
        "    return \" \".join(command)\n",
        "\n",
        "# Function to calculate bitrate\n",
        "def calculate_bitrate(file_path):\n",
        "    command = [\n",
        "    \"ffprobe\",\n",
        "    \"-v\", \"error\",\n",
        "    \"-select_streams\", \"v\",\n",
        "    \"-show_entries\", \"stream=bit_rate\",\n",
        "    \"-of\", \"default=noprint_wrappers=1\",\n",
        "    f\"{file_path}\"\n",
        "    ]\n",
        "    result= subprocess.run(command,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True)\n",
        "    return int(result.stdout.strip().split('=')[1])\n",
        "\n",
        "\n",
        "# Function to get video duration\n",
        "def get_video_resolution(file_path):\n",
        "    result = subprocess.run(\n",
        "        [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v:0\", \"-show_entries\", \"stream=width,height\", \"-of\", \"csv=s=x:p=0\", file_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    result= result.stdout.strip().split('x')\n",
        "    return (int(result[0])//3,int(result[1])//2)\n",
        "\n",
        "# Extract tiles and calculate bitrates\n",
        "\n",
        "width,height=get_video_resolution(input_video)\n",
        "convert_video_to_cubemap(input_video)\n",
        "input_video=input_video.replace('.mp4','_cube.mp4')\n",
        "\"\"\"Change the tile width by setting it here\"\"\"\n",
        "tile_width=width//3\n",
        "output_segments_dir = f\"{input_video.replace('.mp4','_')}segments\"\n",
        "tile_dict={\n",
        "    \"tile_1_left.mp4\":f\"{tile_width}:{height}:0:0\",\n",
        "    \"tile_2_left.mp4\":f\"{tile_width}:{height}:{width}:0\",\n",
        "    \"tile_3_left.mp4\":f\"{tile_width}:{height}:{2*width}:0\",\n",
        "    \"tile_4_left.mp4\":f\"{tile_width}:{height}:0:{height}\",\n",
        "    \"tile_5_left.mp4\":f\"{tile_width}:{height}:{width}:{height}\",\n",
        "    \"tile_6_left.mp4\":f\"{tile_width}:{height}:{2*width}:{height}\",\n",
        "    \"tile_1_right.mp4\":f\"{tile_width}:{height}:{width-tile_width}:0\",\n",
        "    \"tile_2_right.mp4\":f\"{tile_width}:{height}:{2*width-tile_width}:0\",\n",
        "    \"tile_3_right.mp4\":f\"{tile_width}:{height}:{3*width-tile_width}:0\",\n",
        "    \"tile_4_right.mp4\":f\"{tile_width}:{height}:{width-tile_width}:{height}\",\n",
        "    \"tile_5_right.mp4\":f\"{tile_width}:{height}:{2*width-tile_width}:{height}\",\n",
        "    \"tile_6_right.mp4\":f\"{tile_width}:{height}:{3*width-tile_width}:{height}\"\n",
        "}\n",
        "\n",
        "# Split the tile video into 0.5 sec segments\n",
        "get_video_segements(input_video,output_segments_dir)\n",
        "\n",
        "bitrates=[]\n",
        "diff_bitrates=[[0,0,0,0,0,0,0,0,0,0,0,0],]\n",
        "\"\"\"\n",
        "  Structure\n",
        "  [segement_0 values, segment_1 values, ...]\n",
        "  Each segement_no values=list of tile value differences\n",
        "  Calculate bitrate for each segment\n",
        "\"\"\"\n",
        "segments = [f\"output_part_{i}.mp4\" for i in range(0,len(os.listdir(output_segments_dir)))]\n",
        "for index,segment in enumerate(segments):\n",
        "    bitrate_dict={}\n",
        "    diff_bitrate_segment=[]\n",
        "    for tile_name,crop_value in tile_dict.items():\n",
        "        \"\"\"\n",
        "        Extract the tile_width x (height_resolution/2) size tiles\n",
        "        subprocess.run([\"ffmpeg\", \"-i\", input_video, \"-vf\", \"crop=16:16:1024:1536\", output_tile])\n",
        "        \"\"\"\n",
        "        subprocess.run([\"ffmpeg\", \"-i\", output_segments_dir+'/'+segment, \"-vf\", f\"crop={crop_value}\", output_segments_dir+'/'+segment.replace('.mp4','_')+tile_name])\n",
        "        bitrate=calculate_bitrate(output_segments_dir+'/'+segment.replace('.mp4','_')+tile_name)\n",
        "        bitrate_dict[tile_name]=bitrate\n",
        "        if index > 0:\n",
        "             diff_bitrate_segment.append(bitrate - bitrates[index-1][tile_name])\n",
        "    bitrates.append(bitrate_dict)\n",
        "    if index >0 :\n",
        "        diff_bitrates.append(diff_bitrate_segment)\n",
        "for bitrate in bitrates:\n",
        "    print(bitrate)\n",
        "for diff_bitrate in diff_bitrates:\n",
        "    print(diff_bitrate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Jaunpym9F3"
      },
      "source": [
        "Tile index mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7OzLeNSYykLE"
      },
      "outputs": [],
      "source": [
        "index_corr={\n",
        "    0:'tile_1_left.mp4',\n",
        "    1:'tile_2_left.mp4',\n",
        "    2:'tile_3_left.mp4',\n",
        "    3:'tile_4_left.mp4',\n",
        "    4:'tile_5_left.mp4',\n",
        "    5:'tile_6_left.mp4',\n",
        "    6:'tile_1_right.mp4',\n",
        "    7:'tile_2_right.mp4',\n",
        "    8:'tile_3_right.mp4',\n",
        "    9:'tile_4_right.mp4',\n",
        "    10:'tile_5_right.mp4',\n",
        "    11:'tile_6_right.mp4'\n",
        "}\n",
        "normalize_pattern={\n",
        "    0:6,\n",
        "    1:7,\n",
        "    2:8,\n",
        "    3:9,\n",
        "    4:10,\n",
        "    5:11\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTqUoQ7nKGb"
      },
      "source": [
        "Code for calculating the most corelated tiles in the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TefSwLRs-0kn",
        "outputId": "b2de6a89-835d-45ce-91bb-abf116136846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Differences of the most similar values:\n",
            "Segment 5, Curr: tile_2_left.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 5, Curr: tile_5_left.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 5, Curr: tile_2_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 5, Curr: tile_5_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 6, Curr: tile_1_right.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 7, Curr: tile_1_left.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 7, Curr: tile_4_left.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 7, Curr: tile_2_right.mp4, Prev Tile:['tile_4_left.mp4']\n",
            "Segment 8, Curr: tile_6_left.mp4, Prev Tile:['tile_6_right.mp4']\n",
            "Segment 8, Curr: tile_6_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 10, Curr: tile_1_right.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 11, Curr: tile_2_left.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 11, Curr: tile_1_right.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 12, Curr: tile_2_right.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 12, Curr: tile_4_right.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 13, Curr: tile_4_left.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 13, Curr: tile_5_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 14, Curr: tile_6_left.mp4, Prev Tile:['tile_1_right.mp4']\n",
            "Segment 14, Curr: tile_1_right.mp4, Prev Tile:['tile_4_left.mp4']\n",
            "Segment 14, Curr: tile_5_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 14, Curr: tile_6_right.mp4, Prev Tile:['tile_1_right.mp4']\n",
            "Segment 16, Curr: tile_5_left.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 17, Curr: tile_1_right.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 18, Curr: tile_2_left.mp4, Prev Tile:['tile_4_left.mp4']\n",
            "Segment 18, Curr: tile_5_left.mp4, Prev Tile:['tile_2_left.mp4']\n",
            "Segment 18, Curr: tile_1_right.mp4, Prev Tile:['tile_2_left.mp4']\n",
            "Segment 18, Curr: tile_2_right.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 18, Curr: tile_4_right.mp4, Prev Tile:['tile_1_right.mp4']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "most_similar_differences = []\n",
        "most_similar_values=[]\n",
        "threshold=0.15\n",
        "df_diff_bitrates=pd.DataFrame(diff_bitrates)\n",
        "for i in range(2,len(diff_bitrates)):\n",
        "    curr_diff=np.array(diff_bitrates[i])\n",
        "    last_diff=np.array(diff_bitrates[i-1])\n",
        "    # present_dict=pd.DataFrame({'curr_diff':[curr_diff[0]],'last_diff':[last_diff[0]]})\n",
        "    # print(present_dict['curr_diff'].corr(present_dict['last_diff']))\n",
        "    for index,value in enumerate(curr_diff):\n",
        "        \"\"\"\n",
        "        Correlation when all values are positive\n",
        "        differences=np.abs(value * value/np.sqrt(last_diff* last_diff+value*value))\n",
        "        \"\"\"\n",
        "        differences=last_diff/value\n",
        "        \"\"\"Add ignored indexes here\"\"\"\n",
        "        mask=[2,4,8,10]\n",
        "        if(index not in mask):\n",
        "          mask.append(index)\n",
        "        differences[mask]=-100\n",
        "        below_threshold_indices = np.where((differences > 0) & (abs(differences-1) <= threshold) )[0]\n",
        "        if len(below_threshold_indices) > 1:\n",
        "            last_check_segment=i-4 if i>4 else 0\n",
        "            df_corr=df_diff_bitrates[last_check_segment:i+1].corr()\n",
        "            # print(i,index,below_threshold_indices)\n",
        "            # print(df_corr.iloc[index,below_threshold_indices].to_numpy())\n",
        "            below_threshold_indices=[below_threshold_indices[np.argmax(df_corr.iloc[index,below_threshold_indices].to_numpy())]]\n",
        "        if len(below_threshold_indices) > 0:\n",
        "            most_similar_values.append((i,index,below_threshold_indices,differences[below_threshold_indices]))\n",
        "\n",
        "print(\"Differences of the most similar values:\")\n",
        "for most_similar_value in most_similar_values:\n",
        "  print(f\"Segment {most_similar_value[0]}, Curr: {index_corr[most_similar_value[1]]}, Prev Tile:{[index_corr[i] for i in most_similar_value[2]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1n6A5yOFs5P",
        "outputId": "f2fbd72b-6a7e-4c6a-cb33-55d4dfce0207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Differences of the most similar values:\n",
            "Segment 5, Curr: tile_2_left.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 5, Curr: tile_5_left.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 5, Curr: tile_2_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 5, Curr: tile_5_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 6, Curr: tile_1_right.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 7, Curr: tile_1_left.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 7, Curr: tile_4_left.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 7, Curr: tile_2_right.mp4, Prev Tile:['tile_4_left.mp4']\n",
            "Segment 8, Curr: tile_6_left.mp4, Prev Tile:['tile_6_right.mp4']\n",
            "Segment 8, Curr: tile_6_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 10, Curr: tile_1_right.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 11, Curr: tile_2_left.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 11, Curr: tile_1_right.mp4, Prev Tile:['tile_1_left.mp4']\n",
            "Segment 12, Curr: tile_2_right.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 12, Curr: tile_4_right.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 13, Curr: tile_4_left.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 13, Curr: tile_5_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 14, Curr: tile_6_left.mp4, Prev Tile:['tile_1_right.mp4']\n",
            "Segment 14, Curr: tile_1_right.mp4, Prev Tile:['tile_4_left.mp4']\n",
            "Segment 14, Curr: tile_5_right.mp4, Prev Tile:['tile_6_left.mp4']\n",
            "Segment 14, Curr: tile_6_right.mp4, Prev Tile:['tile_1_right.mp4']\n",
            "Segment 16, Curr: tile_5_left.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 17, Curr: tile_1_right.mp4, Prev Tile:['tile_2_right.mp4']\n",
            "Segment 18, Curr: tile_2_left.mp4, Prev Tile:['tile_4_left.mp4']\n",
            "Segment 18, Curr: tile_5_left.mp4, Prev Tile:['tile_2_left.mp4']\n",
            "Segment 18, Curr: tile_1_right.mp4, Prev Tile:['tile_2_left.mp4']\n",
            "Segment 18, Curr: tile_2_right.mp4, Prev Tile:['tile_4_right.mp4']\n",
            "Segment 18, Curr: tile_4_right.mp4, Prev Tile:['tile_1_right.mp4']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "norm_diff=scaler.fit_transform(diff_bitrates)\n",
        "df_norm_diff_bitrates=pd.DataFrame(diff_bitrates)\n",
        "\n",
        "most_similar_differences = []\n",
        "most_similar_values=[]\n",
        "threshold=0.15\n",
        "for i in range(2,len(diff_bitrates)):\n",
        "    curr_diff=np.array(diff_bitrates[i])\n",
        "    last_diff=np.array(diff_bitrates[i-1])\n",
        "    for index,value in enumerate(curr_diff):\n",
        "        differences=last_diff/value\n",
        "        # differences[index]=1e10\n",
        "        mask=[2,4,8,10]\n",
        "        if(index not in mask):\n",
        "          mask.append(index)\n",
        "        differences[mask]=-100\n",
        "        # min_index=np.argmin(differences)\n",
        "        below_threshold_indices = np.where((differences > 0) & (abs(differences-1) <= threshold) )[0]\n",
        "        if len(below_threshold_indices) > 1:\n",
        "            last_check_segment=i-4 if i>4 else 0\n",
        "            df_corr=df_norm_diff_bitrates[last_check_segment:i+1].corr()\n",
        "            # print(i,index,below_threshold_indices)\n",
        "            # print(df_corr.iloc[index,below_threshold_indices].to_numpy())\n",
        "            below_threshold_indices=[below_threshold_indices[np.argmax(df_corr.iloc[index,below_threshold_indices].to_numpy())]]\n",
        "        if len(below_threshold_indices) > 0:\n",
        "            most_similar_values.append((i,index,below_threshold_indices,differences[below_threshold_indices]))\n",
        "        # most_similar_differences.append(differences[min_index])\n",
        "\n",
        "print(\"Differences of the most similar values:\")\n",
        "for most_similar_value in most_similar_values:\n",
        "  print(f\"Segment {most_similar_value[0]}, Curr: {index_corr[most_similar_value[1]]}, Prev Tile:{[index_corr[i] for i in most_similar_value[2]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAH-Zy1DgZx4",
        "outputId": "afb69a75-9f27-4ff7-d13e-0e3a277a1d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.49-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.49-py3-none-any.whl (898 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.49 ultralytics-thop-2.0.13\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1L62BLWf97N",
        "outputId": "59917bd2-5dce-4b58-a195-94a30ec13773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21.5M/21.5M [00:00<00:00, 121MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "Collecting lap>=0.5.12\n",
            "  Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lap>=0.5.12) (1.26.4)\n",
            "Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 21.3 MB/s eta 0:00:00\n",
            "Installing collected packages: lap\n",
            "Successfully installed lap-0.5.12\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 2.6s, installed 1 package: ['lap>=0.5.12']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 415.7ms\n",
            "Speed: 15.4ms preprocess, 415.7ms inference, 23.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 293.1ms\n",
            "Speed: 2.6ms preprocess, 293.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 291.8ms\n",
            "Speed: 2.0ms preprocess, 291.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 300.2ms\n",
            "Speed: 2.9ms preprocess, 300.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 309.3ms\n",
            "Speed: 2.9ms preprocess, 309.3ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 293.9ms\n",
            "Speed: 2.8ms preprocess, 293.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 303.7ms\n",
            "Speed: 2.0ms preprocess, 303.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 389.8ms\n",
            "Speed: 2.7ms preprocess, 389.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 390.6ms\n",
            "Speed: 6.2ms preprocess, 390.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 367.4ms\n",
            "Speed: 2.6ms preprocess, 367.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 397.8ms\n",
            "Speed: 4.1ms preprocess, 397.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 383.7ms\n",
            "Speed: 2.7ms preprocess, 383.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 282.6ms\n",
            "Speed: 2.1ms preprocess, 282.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 290.0ms\n",
            "Speed: 2.9ms preprocess, 290.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 286.5ms\n",
            "Speed: 2.2ms preprocess, 286.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 276.6ms\n",
            "Speed: 2.0ms preprocess, 276.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 287.7ms\n",
            "Speed: 2.6ms preprocess, 287.7ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 292.7ms\n",
            "Speed: 2.9ms preprocess, 292.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 297.1ms\n",
            "Speed: 2.0ms preprocess, 297.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 291.7ms\n",
            "Speed: 2.8ms preprocess, 291.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 294.6ms\n",
            "Speed: 2.8ms preprocess, 294.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 288.1ms\n",
            "Speed: 3.0ms preprocess, 288.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 284.9ms\n",
            "Speed: 2.2ms preprocess, 284.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 291.8ms\n",
            "Speed: 2.9ms preprocess, 291.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 303.4ms\n",
            "Speed: 2.2ms preprocess, 303.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 291.2ms\n",
            "Speed: 2.8ms preprocess, 291.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 282.0ms\n",
            "Speed: 2.8ms preprocess, 282.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 298.7ms\n",
            "Speed: 2.0ms preprocess, 298.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 292.7ms\n",
            "Speed: 3.3ms preprocess, 292.7ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 297.8ms\n",
            "Speed: 2.9ms preprocess, 297.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 305.3ms\n",
            "Speed: 5.7ms preprocess, 305.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 14 persons, 1 dog, 1 handbag, 278.7ms\n",
            "Speed: 2.2ms preprocess, 278.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 1 dog, 310.8ms\n",
            "Speed: 2.8ms preprocess, 310.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 285.4ms\n",
            "Speed: 2.9ms preprocess, 285.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 291.9ms\n",
            "Speed: 2.8ms preprocess, 291.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 dog, 296.0ms\n",
            "Speed: 2.7ms preprocess, 296.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 1 bicycle, 1 dog, 300.2ms\n",
            "Speed: 2.9ms preprocess, 300.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 1 bicycle, 1 dog, 297.3ms\n",
            "Speed: 3.1ms preprocess, 297.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 bicycle, 1 dog, 389.7ms\n",
            "Speed: 2.6ms preprocess, 389.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 bicycle, 1 dog, 365.5ms\n",
            "Speed: 2.5ms preprocess, 365.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 bicycle, 345.4ms\n",
            "Speed: 10.0ms preprocess, 345.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 2 bicycles, 1 dog, 1 backpack, 393.5ms\n",
            "Speed: 2.5ms preprocess, 393.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 2 bicycles, 1 dog, 1 handbag, 389.2ms\n",
            "Speed: 7.5ms preprocess, 389.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 2 bicycles, 1 dog, 317.7ms\n",
            "Speed: 2.7ms preprocess, 317.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 2 bicycles, 1 dog, 290.9ms\n",
            "Speed: 2.9ms preprocess, 290.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 bicycles, 1 dog, 283.6ms\n",
            "Speed: 2.9ms preprocess, 283.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 bicycles, 1 dog, 289.6ms\n",
            "Speed: 2.7ms preprocess, 289.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 bicycle, 1 dog, 287.7ms\n",
            "Speed: 2.7ms preprocess, 287.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 bicycle, 1 dog, 1 backpack, 291.8ms\n",
            "Speed: 2.8ms preprocess, 291.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 bicycle, 1 backpack, 288.4ms\n",
            "Speed: 2.7ms preprocess, 288.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 1 bicycle, 1 handbag, 273.7ms\n",
            "Speed: 2.9ms preprocess, 273.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 14 persons, 1 bicycle, 1 handbag, 279.2ms\n",
            "Speed: 2.8ms preprocess, 279.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 14 persons, 1 bicycle, 286.3ms\n",
            "Speed: 2.8ms preprocess, 286.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 276.8ms\n",
            "Speed: 3.4ms preprocess, 276.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 283.5ms\n",
            "Speed: 2.8ms preprocess, 283.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 278.3ms\n",
            "Speed: 2.0ms preprocess, 278.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 289.6ms\n",
            "Speed: 2.7ms preprocess, 289.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 287.4ms\n",
            "Speed: 2.7ms preprocess, 287.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 dog, 280.7ms\n",
            "Speed: 2.8ms preprocess, 280.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 272.4ms\n",
            "Speed: 2.9ms preprocess, 272.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 297.5ms\n",
            "Speed: 2.7ms preprocess, 297.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 275.9ms\n",
            "Speed: 2.7ms preprocess, 275.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 backpack, 279.7ms\n",
            "Speed: 2.0ms preprocess, 279.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 284.3ms\n",
            "Speed: 2.8ms preprocess, 284.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 290.6ms\n",
            "Speed: 2.8ms preprocess, 290.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 285.0ms\n",
            "Speed: 2.7ms preprocess, 285.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 286.4ms\n",
            "Speed: 2.8ms preprocess, 286.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 280.9ms\n",
            "Speed: 2.7ms preprocess, 280.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 290.2ms\n",
            "Speed: 2.7ms preprocess, 290.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 288.1ms\n",
            "Speed: 2.8ms preprocess, 288.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 365.1ms\n",
            "Speed: 2.4ms preprocess, 365.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 352.1ms\n",
            "Speed: 7.3ms preprocess, 352.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 362.0ms\n",
            "Speed: 3.0ms preprocess, 362.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 402.4ms\n",
            "Speed: 3.3ms preprocess, 402.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 388.5ms\n",
            "Speed: 2.7ms preprocess, 388.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 6 persons, 315.1ms\n",
            "Speed: 4.6ms preprocess, 315.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 276.8ms\n",
            "Speed: 2.9ms preprocess, 276.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 290.9ms\n",
            "Speed: 2.4ms preprocess, 290.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 1 bus, 283.6ms\n",
            "Speed: 2.9ms preprocess, 283.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 283.2ms\n",
            "Speed: 2.7ms preprocess, 283.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 6 persons, 1 car, 287.2ms\n",
            "Speed: 3.0ms preprocess, 287.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 287.5ms\n",
            "Speed: 2.0ms preprocess, 287.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 289.9ms\n",
            "Speed: 2.7ms preprocess, 289.9ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 283.5ms\n",
            "Speed: 3.7ms preprocess, 283.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 286.8ms\n",
            "Speed: 2.8ms preprocess, 286.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 290.5ms\n",
            "Speed: 2.9ms preprocess, 290.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 282.3ms\n",
            "Speed: 2.7ms preprocess, 282.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 282.1ms\n",
            "Speed: 2.8ms preprocess, 282.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 299.5ms\n",
            "Speed: 4.0ms preprocess, 299.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 507.8ms\n",
            "Speed: 2.7ms preprocess, 507.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 285.0ms\n",
            "Speed: 1.7ms preprocess, 285.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 445.8ms\n",
            "Speed: 2.5ms preprocess, 445.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 436.0ms\n",
            "Speed: 2.8ms preprocess, 436.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 281.3ms\n",
            "Speed: 2.9ms preprocess, 281.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 288.7ms\n",
            "Speed: 2.8ms preprocess, 288.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 287.0ms\n",
            "Speed: 2.8ms preprocess, 287.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 295.1ms\n",
            "Speed: 2.8ms preprocess, 295.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 284.4ms\n",
            "Speed: 2.8ms preprocess, 284.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 6 persons, 1 car, 290.8ms\n",
            "Speed: 2.8ms preprocess, 290.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 6 persons, 1 car, 280.4ms\n",
            "Speed: 2.7ms preprocess, 280.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 383.9ms\n",
            "Speed: 3.2ms preprocess, 383.9ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 368.0ms\n",
            "Speed: 9.4ms preprocess, 368.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 6 persons, 1 car, 371.8ms\n",
            "Speed: 2.9ms preprocess, 371.8ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 380.5ms\n",
            "Speed: 2.8ms preprocess, 380.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 397.5ms\n",
            "Speed: 2.5ms preprocess, 397.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 325.5ms\n",
            "Speed: 3.5ms preprocess, 325.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 293.3ms\n",
            "Speed: 2.8ms preprocess, 293.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 283.1ms\n",
            "Speed: 2.2ms preprocess, 283.1ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 301.4ms\n",
            "Speed: 2.8ms preprocess, 301.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 295.5ms\n",
            "Speed: 2.7ms preprocess, 295.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 3 cars, 276.0ms\n",
            "Speed: 2.2ms preprocess, 276.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 272.9ms\n",
            "Speed: 1.8ms preprocess, 272.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 296.1ms\n",
            "Speed: 2.7ms preprocess, 296.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 303.9ms\n",
            "Speed: 6.0ms preprocess, 303.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 297.9ms\n",
            "Speed: 2.9ms preprocess, 297.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 289.4ms\n",
            "Speed: 3.2ms preprocess, 289.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 305.6ms\n",
            "Speed: 2.9ms preprocess, 305.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 299.1ms\n",
            "Speed: 2.9ms preprocess, 299.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 280.8ms\n",
            "Speed: 2.8ms preprocess, 280.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 6 persons, 1 car, 271.8ms\n",
            "Speed: 2.7ms preprocess, 271.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 291.8ms\n",
            "Speed: 7.5ms preprocess, 291.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 5 persons, 1 bicycle, 1 car, 280.6ms\n",
            "Speed: 2.9ms preprocess, 280.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 bicycle, 1 car, 287.7ms\n",
            "Speed: 2.8ms preprocess, 287.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 bicycle, 1 car, 282.6ms\n",
            "Speed: 5.3ms preprocess, 282.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 288.5ms\n",
            "Speed: 2.9ms preprocess, 288.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 293.1ms\n",
            "Speed: 2.9ms preprocess, 293.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 286.2ms\n",
            "Speed: 3.0ms preprocess, 286.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 269.7ms\n",
            "Speed: 2.8ms preprocess, 269.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 291.6ms\n",
            "Speed: 2.8ms preprocess, 291.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 281.8ms\n",
            "Speed: 2.7ms preprocess, 281.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 279.9ms\n",
            "Speed: 2.7ms preprocess, 279.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 289.0ms\n",
            "Speed: 1.9ms preprocess, 289.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 360.2ms\n",
            "Speed: 2.7ms preprocess, 360.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 1 motorcycle, 388.6ms\n",
            "Speed: 9.6ms preprocess, 388.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 340.3ms\n",
            "Speed: 9.6ms preprocess, 340.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 372.1ms\n",
            "Speed: 5.5ms preprocess, 372.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 2 cars, 1 motorcycle, 396.2ms\n",
            "Speed: 4.8ms preprocess, 396.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 292.5ms\n",
            "Speed: 2.8ms preprocess, 292.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 288.5ms\n",
            "Speed: 2.8ms preprocess, 288.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 277.8ms\n",
            "Speed: 2.0ms preprocess, 277.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 297.9ms\n",
            "Speed: 2.7ms preprocess, 297.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 281.2ms\n",
            "Speed: 2.8ms preprocess, 281.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 277.0ms\n",
            "Speed: 2.6ms preprocess, 277.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 16 persons, 2 cars, 281.0ms\n",
            "Speed: 2.8ms preprocess, 281.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 2 cars, 278.7ms\n",
            "Speed: 2.7ms preprocess, 278.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 2 cars, 286.6ms\n",
            "Speed: 2.6ms preprocess, 286.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 2 cars, 1 truck, 283.1ms\n",
            "Speed: 2.0ms preprocess, 283.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 3 cars, 282.1ms\n",
            "Speed: 2.2ms preprocess, 282.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 3 cars, 292.0ms\n",
            "Speed: 2.8ms preprocess, 292.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 3 cars, 278.5ms\n",
            "Speed: 2.7ms preprocess, 278.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 3 cars, 1 boat, 278.3ms\n",
            "Speed: 2.2ms preprocess, 278.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 1 boat, 290.2ms\n",
            "Speed: 2.5ms preprocess, 290.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 3 cars, 283.7ms\n",
            "Speed: 2.7ms preprocess, 283.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 1 truck, 281.1ms\n",
            "Speed: 4.0ms preprocess, 281.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 278.3ms\n",
            "Speed: 1.9ms preprocess, 278.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 272.8ms\n",
            "Speed: 2.2ms preprocess, 272.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 290.6ms\n",
            "Speed: 2.7ms preprocess, 290.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 3 cars, 278.1ms\n",
            "Speed: 2.9ms preprocess, 278.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 14 persons, 3 cars, 1 motorcycle, 281.8ms\n",
            "Speed: 2.8ms preprocess, 281.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 cars, 291.9ms\n",
            "Speed: 2.8ms preprocess, 291.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 cars, 290.9ms\n",
            "Speed: 2.7ms preprocess, 290.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 282.4ms\n",
            "Speed: 2.6ms preprocess, 282.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 281.9ms\n",
            "Speed: 6.4ms preprocess, 281.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 bicycle, 1 car, 291.6ms\n",
            "Speed: 2.9ms preprocess, 291.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 366.2ms\n",
            "Speed: 6.7ms preprocess, 366.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 bicycle, 1 car, 370.4ms\n",
            "Speed: 2.8ms preprocess, 370.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 bicycle, 1 car, 389.5ms\n",
            "Speed: 4.5ms preprocess, 389.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 bicycle, 1 car, 369.3ms\n",
            "Speed: 2.6ms preprocess, 369.3ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 344.2ms\n",
            "Speed: 3.3ms preprocess, 344.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 280.3ms\n",
            "Speed: 2.9ms preprocess, 280.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 277.8ms\n",
            "Speed: 2.6ms preprocess, 277.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 289.0ms\n",
            "Speed: 2.7ms preprocess, 289.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 281.7ms\n",
            "Speed: 1.9ms preprocess, 281.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 276.7ms\n",
            "Speed: 2.8ms preprocess, 276.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 298.7ms\n",
            "Speed: 4.2ms preprocess, 298.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 279.7ms\n",
            "Speed: 2.9ms preprocess, 279.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 280.3ms\n",
            "Speed: 2.7ms preprocess, 280.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 umbrella, 1 handbag, 277.3ms\n",
            "Speed: 2.6ms preprocess, 277.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 1 umbrella, 1 handbag, 276.6ms\n",
            "Speed: 2.7ms preprocess, 276.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 280.7ms\n",
            "Speed: 2.9ms preprocess, 280.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 279.2ms\n",
            "Speed: 2.7ms preprocess, 279.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 281.1ms\n",
            "Speed: 2.5ms preprocess, 281.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 300.0ms\n",
            "Speed: 2.7ms preprocess, 300.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 1 umbrella, 282.8ms\n",
            "Speed: 2.6ms preprocess, 282.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 1 umbrella, 292.8ms\n",
            "Speed: 2.8ms preprocess, 292.8ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 1 umbrella, 1 handbag, 286.1ms\n",
            "Speed: 1.9ms preprocess, 286.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 6 persons, 1 car, 1 umbrella, 277.1ms\n",
            "Speed: 2.6ms preprocess, 277.1ms inference, 2.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 1 umbrella, 1 handbag, 277.5ms\n",
            "Speed: 3.3ms preprocess, 277.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 1 umbrella, 279.8ms\n",
            "Speed: 2.7ms preprocess, 279.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 1 umbrella, 280.6ms\n",
            "Speed: 2.0ms preprocess, 280.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 1 umbrella, 292.5ms\n",
            "Speed: 2.7ms preprocess, 292.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 umbrella, 280.9ms\n",
            "Speed: 3.2ms preprocess, 280.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 boat, 289.1ms\n",
            "Speed: 2.7ms preprocess, 289.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 1 car, 1 boat, 2 umbrellas, 294.1ms\n",
            "Speed: 2.7ms preprocess, 294.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 1 boat, 1 umbrella, 289.0ms\n",
            "Speed: 2.7ms preprocess, 289.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 1 umbrella, 379.8ms\n",
            "Speed: 2.7ms preprocess, 379.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 1 umbrella, 359.2ms\n",
            "Speed: 2.6ms preprocess, 359.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 1 umbrella, 362.7ms\n",
            "Speed: 3.3ms preprocess, 362.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 1 boat, 392.9ms\n",
            "Speed: 7.5ms preprocess, 392.9ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 1 umbrella, 369.3ms\n",
            "Speed: 2.8ms preprocess, 369.3ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 1 umbrella, 1 handbag, 325.1ms\n",
            "Speed: 3.7ms preprocess, 325.1ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 handbag, 287.6ms\n",
            "Speed: 2.1ms preprocess, 287.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 handbag, 295.9ms\n",
            "Speed: 2.7ms preprocess, 295.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 280.3ms\n",
            "Speed: 2.8ms preprocess, 280.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 283.3ms\n",
            "Speed: 2.7ms preprocess, 283.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 umbrellas, 282.7ms\n",
            "Speed: 2.3ms preprocess, 282.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 umbrellas, 1 handbag, 286.0ms\n",
            "Speed: 2.7ms preprocess, 286.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 3 umbrellas, 280.7ms\n",
            "Speed: 2.8ms preprocess, 280.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 3 umbrellas, 279.0ms\n",
            "Speed: 2.9ms preprocess, 279.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 umbrellas, 1 handbag, 301.2ms\n",
            "Speed: 2.5ms preprocess, 301.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 umbrella, 297.9ms\n",
            "Speed: 6.6ms preprocess, 297.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 1 umbrella, 294.2ms\n",
            "Speed: 2.7ms preprocess, 294.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 285.4ms\n",
            "Speed: 2.9ms preprocess, 285.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 283.9ms\n",
            "Speed: 2.7ms preprocess, 283.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 umbrella, 288.3ms\n",
            "Speed: 2.7ms preprocess, 288.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 umbrella, 1 handbag, 278.3ms\n",
            "Speed: 2.7ms preprocess, 278.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 cars, 1 handbag, 286.5ms\n",
            "Speed: 2.6ms preprocess, 286.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 1 handbag, 1 chair, 279.2ms\n",
            "Speed: 2.7ms preprocess, 279.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 3 cars, 1 handbag, 1 chair, 285.4ms\n",
            "Speed: 2.7ms preprocess, 285.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 3 cars, 1 umbrella, 1 chair, 287.7ms\n",
            "Speed: 2.8ms preprocess, 287.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 1 handbag, 282.5ms\n",
            "Speed: 2.9ms preprocess, 282.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 1 truck, 291.8ms\n",
            "Speed: 2.4ms preprocess, 291.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 1 boat, 1 umbrella, 1 handbag, 292.0ms\n",
            "Speed: 2.7ms preprocess, 292.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 boat, 1 umbrella, 2 handbags, 281.4ms\n",
            "Speed: 2.7ms preprocess, 281.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 1 umbrella, 1 handbag, 283.5ms\n",
            "Speed: 2.9ms preprocess, 283.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 4 handbags, 287.6ms\n",
            "Speed: 2.9ms preprocess, 287.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 1 truck, 2 handbags, 283.3ms\n",
            "Speed: 2.7ms preprocess, 283.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 4 cars, 1 boat, 871.7ms\n",
            "Speed: 4.7ms preprocess, 871.7ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 1 boat, 819.1ms\n",
            "Speed: 2.6ms preprocess, 819.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 894.0ms\n",
            "Speed: 10.0ms preprocess, 894.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 2 cars, 515.2ms\n",
            "Speed: 2.6ms preprocess, 515.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 3 cars, 292.0ms\n",
            "Speed: 2.7ms preprocess, 292.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 3 cars, 286.6ms\n",
            "Speed: 2.8ms preprocess, 286.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 3 cars, 285.7ms\n",
            "Speed: 2.6ms preprocess, 285.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 1 boat, 293.4ms\n",
            "Speed: 2.7ms preprocess, 293.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 3 cars, 2 handbags, 279.0ms\n",
            "Speed: 2.6ms preprocess, 279.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 cars, 1 handbag, 293.5ms\n",
            "Speed: 2.8ms preprocess, 293.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 car, 1 boat, 1 handbag, 293.5ms\n",
            "Speed: 2.7ms preprocess, 293.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 car, 1 boat, 1 handbag, 284.6ms\n",
            "Speed: 2.7ms preprocess, 284.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 1 boat, 1 handbag, 294.1ms\n",
            "Speed: 2.9ms preprocess, 294.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 1 boat, 2 handbags, 285.2ms\n",
            "Speed: 2.7ms preprocess, 285.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 cars, 1 boat, 1 handbag, 290.0ms\n",
            "Speed: 2.7ms preprocess, 290.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 1 boat, 1 handbag, 284.9ms\n",
            "Speed: 2.5ms preprocess, 284.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 1 boat, 2 handbags, 276.2ms\n",
            "Speed: 2.8ms preprocess, 276.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 1 truck, 1 boat, 1 handbag, 292.1ms\n",
            "Speed: 2.6ms preprocess, 292.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 3 cars, 1 truck, 1 boat, 1 handbag, 286.9ms\n",
            "Speed: 2.7ms preprocess, 286.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 4 cars, 2 boats, 1 handbag, 284.7ms\n",
            "Speed: 3.2ms preprocess, 284.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 2 cars, 2 boats, 1 handbag, 304.4ms\n",
            "Speed: 2.7ms preprocess, 304.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 cars, 2 boats, 1 handbag, 282.0ms\n",
            "Speed: 2.7ms preprocess, 282.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 4 cars, 2 boats, 1 handbag, 280.1ms\n",
            "Speed: 2.8ms preprocess, 280.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 4 cars, 1 boat, 1 handbag, 292.1ms\n",
            "Speed: 2.8ms preprocess, 292.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 4 cars, 1 train, 1 boat, 1 handbag, 283.3ms\n",
            "Speed: 2.8ms preprocess, 283.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 4 cars, 2 boats, 298.2ms\n",
            "Speed: 2.8ms preprocess, 298.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 4 cars, 1 train, 1 boat, 285.4ms\n",
            "Speed: 2.9ms preprocess, 285.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 4 cars, 1 train, 1 boat, 285.0ms\n",
            "Speed: 2.7ms preprocess, 285.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 3 cars, 1 train, 1 boat, 1 handbag, 390.3ms\n",
            "Speed: 2.8ms preprocess, 390.3ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 1 train, 1 boat, 1 handbag, 354.5ms\n",
            "Speed: 4.5ms preprocess, 354.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 13 persons, 2 cars, 1 train, 1 boat, 1 handbag, 350.8ms\n",
            "Speed: 3.0ms preprocess, 350.8ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 2 cars, 1 train, 1 boat, 1 handbag, 386.1ms\n",
            "Speed: 9.1ms preprocess, 386.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 cars, 1 train, 393.2ms\n",
            "Speed: 2.7ms preprocess, 393.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 6 cars, 1 boat, 309.7ms\n",
            "Speed: 2.7ms preprocess, 309.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 4 cars, 1 train, 286.7ms\n",
            "Speed: 2.5ms preprocess, 286.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 4 cars, 1 handbag, 288.8ms\n",
            "Speed: 3.0ms preprocess, 288.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 4 cars, 1 handbag, 283.9ms\n",
            "Speed: 2.8ms preprocess, 283.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 7 cars, 1 handbag, 292.0ms\n",
            "Speed: 2.4ms preprocess, 292.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 4 cars, 279.5ms\n",
            "Speed: 2.7ms preprocess, 279.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 1 car, 1 train, 1 handbag, 288.1ms\n",
            "Speed: 2.6ms preprocess, 288.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 train, 1 handbag, 283.9ms\n",
            "Speed: 2.7ms preprocess, 283.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 train, 2 handbags, 279.8ms\n",
            "Speed: 2.7ms preprocess, 279.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 1 car, 1 train, 2 handbags, 281.0ms\n",
            "Speed: 2.5ms preprocess, 281.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 14 persons, 1 train, 2 handbags, 284.9ms\n",
            "Speed: 2.7ms preprocess, 284.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 1 car, 1 train, 1 handbag, 280.1ms\n",
            "Speed: 2.8ms preprocess, 280.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 1 train, 1 handbag, 297.6ms\n",
            "Speed: 2.8ms preprocess, 297.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 train, 1 handbag, 299.0ms\n",
            "Speed: 2.7ms preprocess, 299.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 train, 373.2ms\n",
            "Speed: 2.5ms preprocess, 373.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 1 train, 1 handbag, 358.0ms\n",
            "Speed: 3.1ms preprocess, 358.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 351.6ms\n",
            "Speed: 2.7ms preprocess, 351.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 1 handbag, 380.2ms\n",
            "Speed: 2.9ms preprocess, 380.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 1 handbag, 387.6ms\n",
            "Speed: 2.5ms preprocess, 387.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 car, 346.3ms\n",
            "Speed: 2.7ms preprocess, 346.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 287.4ms\n",
            "Speed: 2.8ms preprocess, 287.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 2 cars, 291.8ms\n",
            "Speed: 4.5ms preprocess, 291.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 6 cars, 274.2ms\n",
            "Speed: 2.7ms preprocess, 274.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 7 cars, 281.6ms\n",
            "Speed: 3.1ms preprocess, 281.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 5 cars, 280.8ms\n",
            "Speed: 2.7ms preprocess, 280.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 14 persons, 5 cars, 1 skateboard, 369.5ms\n",
            "Speed: 4.6ms preprocess, 369.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 7 cars, 1 skateboard, 358.9ms\n",
            "Speed: 2.6ms preprocess, 358.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 6 cars, 1 horse, 1 skateboard, 342.4ms\n",
            "Speed: 5.0ms preprocess, 342.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 5 cars, 1 skateboard, 375.7ms\n",
            "Speed: 2.7ms preprocess, 375.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 6 cars, 1 skateboard, 374.8ms\n",
            "Speed: 2.5ms preprocess, 374.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 6 cars, 1 skateboard, 313.4ms\n",
            "Speed: 6.3ms preprocess, 313.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 5 cars, 1 skateboard, 290.9ms\n",
            "Speed: 2.3ms preprocess, 290.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 11 persons, 6 cars, 1 cow, 275.3ms\n",
            "Speed: 2.7ms preprocess, 275.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 6 cars, 1 cow, 277.4ms\n",
            "Speed: 2.7ms preprocess, 277.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 10 persons, 4 cars, 276.3ms\n",
            "Speed: 2.9ms preprocess, 276.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 6 cars, 286.0ms\n",
            "Speed: 2.7ms preprocess, 286.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 4 cars, 1 truck, 276.0ms\n",
            "Speed: 2.3ms preprocess, 276.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 5 cars, 287.6ms\n",
            "Speed: 2.8ms preprocess, 287.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 5 cars, 281.6ms\n",
            "Speed: 2.7ms preprocess, 281.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 8 persons, 4 cars, 286.5ms\n",
            "Speed: 1.9ms preprocess, 286.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 3 cars, 278.0ms\n",
            "Speed: 1.7ms preprocess, 278.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 5 cars, 274.0ms\n",
            "Speed: 2.0ms preprocess, 274.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the model\n",
        "yolo = YOLO('yolov8s.pt')\n",
        "\n",
        "# Load the video capture (Replace 'video.mp4' with the path to your video file)\n",
        "videoCap = cv2.VideoCapture(input_video)\n",
        "\n",
        "# Get video properties\n",
        "frame_width = int(videoCap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(videoCap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(videoCap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define codec and create VideoWriter object (output file: 'output.mp4')\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
        "output = cv2.VideoWriter(input_video.replace('.mp4','_yolo.mp4'), fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Function to get class colors\n",
        "def getColours(cls_num):\n",
        "    base_colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "    color_index = cls_num % len(base_colors)\n",
        "    increments = [(1, -2, 1), (-2, 1, -1), (1, -1, 2)]\n",
        "    color = [base_colors[color_index][i] + increments[color_index][i] *\n",
        "    (cls_num // len(base_colors)) % 256 for i in range(3)]\n",
        "    return tuple(color)\n",
        "\n",
        "while True:\n",
        "    ret, frame = videoCap.read()\n",
        "    if not ret:\n",
        "        break  # Exit loop if the video ends\n",
        "\n",
        "    results = yolo.track(frame, stream=True)\n",
        "\n",
        "    for result in results:\n",
        "        # get the classes names\n",
        "        classes_names = result.names\n",
        "\n",
        "        # iterate over each box\n",
        "        for box in result.boxes:\n",
        "            # check if confidence is greater than 40 percent\n",
        "            if box.conf[0] > 0.4:\n",
        "                # get coordinates\n",
        "                [x1, y1, x2, y2] = box.xyxy[0]\n",
        "                # convert to int\n",
        "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "                # get the class\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                # get the class name\n",
        "                class_name = classes_names[cls]\n",
        "\n",
        "                # get the respective colour\n",
        "                colour = getColours(cls)\n",
        "\n",
        "                # draw the rectangle\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), colour, 2)\n",
        "\n",
        "                # put the class name and confidence on the image\n",
        "                cv2.putText(frame, f'{classes_names[int(box.cls[0])]} {box.conf[0]:.2f}', (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, colour, 2)\n",
        "\n",
        "    # Write the processed frame to the output video file\n",
        "    output.write(frame)\n",
        "\n",
        "# Release resources\n",
        "videoCap.release()\n",
        "output.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM4Hyw2iJGBB",
        "outputId": "a3741c73-e7a8-4b47-a07b-05bcd590d2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[    0.28571        0.68]\n",
            " [    0.42857        0.76]\n",
            " [        0.5        0.28]\n",
            " [          1           0]\n",
            " [          0        0.56]\n",
            " [    0.28571           1]]\n",
            "[[2, 4], [4, 6], [5, -6], [12, -13], [-2, 1], [2, 12]]\n",
            "[[          0     0.60714]\n",
            " [    0.27273     0.67857]\n",
            " [    0.36364        0.25]\n",
            " [          1           0]\n",
            " [   0.090909         0.5]\n",
            " [   0.090909           1]]\n",
            "[[1, 4], [4, 6], [5, -6], [12, -13], [2, 1], [2, 15]]\n",
            "[[          0           0]\n",
            " [        0.2         0.2]\n",
            " [        0.4         0.4]\n",
            " [        0.6         0.6]\n",
            " [        0.8         0.8]\n",
            " [          1           1]]\n",
            "[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from scipy.signal import coherence\n",
        "#normalizing bitrate values per tile\n",
        "scaler=MinMaxScaler()\n",
        "i=0\n",
        "temp=[[2,4,5,12,-2,2,4,6,-6,-13,1,12],[1,4,5,12,2,2,4,6,-6,-13,1,15],[0,2,4,6,8,10,1,3,5,7,9,11]]\n",
        "for diff_bitrate in temp:\n",
        "    tile_set=[]\n",
        "    for left_tile,right_tile in normalize_pattern.items():\n",
        "        tile_set.append([diff_bitrate[left_tile],diff_bitrate[right_tile]])\n",
        "        # tile_set=np.array([[diff_bitrate[left_tile]],[diff_bitrate[tile_right]]])\n",
        "        # normalized_tiles_values=scaler.fit_transform(tile_set)\n",
        "        # print(tile_set,normalized_tiles_values)\n",
        "    normalized_tiles_values=scaler.fit_transform(np.array(tile_set))\n",
        "    print(normalized_tiles_values)\n",
        "    print(tile_set)\n",
        "    i+=1\n",
        "    if i>2:\n",
        "      break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
